
---
title: 说说redis的数据过期策略和内存淘汰机制
tags: redis,读书笔记
---

## redis的过期策略

在我们设置redis key 的时候,一般会设置一个过期时间.不设置过期时间的情况下,默认是永不过期.

所谓的`过期策略`是指,当redis中的key过期时,redis如何处理.



redis中使用的过期策略有两种 :

1. **惰性删除**

   当key过期时,不做处理.每次从数据库获取key时,检查key是否过期.若过期,则删除,返回null

   惰性删除对cpu是友好的,因为删除操作只在获取key时发生.

   它的缺点是,对内存够不友好.如果有大量的key长时间没有被访问,那么这些key就永远不会被删除.可能因此导致内存泄露

为了弥补惰性删除的缺点,redis中还引入了定期删除的策略.

2. **定期删除**

   每隔一段时间(默认100ms)随机抽取一部分设置了过期时间的key,若过期,则删除.
   
   定期删除弥补了`惰性删除`的缺点.Redis会通过定期删除策略定期主动淘汰一批已过期的key.
   


## redis 的内存淘汰策略

当redis达到内存最大限制时,redis可能会自动清除旧的数据. 默认情况下 memcached 使用的就是这种方式.

而redis会根据配置的淘汰策略,来决定具体的行为.



当前版本,redis支持6种内存淘汰策略:

* **不删除策略(noeviction)** 

  默认的内存淘汰策略.在达到内存最大限制时,如果执行了需要更多内存的操作,则直接返回错误信息.

  大多数写命令都会导致占用更多的内存.少数命令(如 DEL )例外.

* **优先删除最近少使用的key策略(allkeys-lru)**

  在达到内存最大限制时,通过近似LRU算法删除最近最少使用的key.

* **优先删除最近少使用的带过期时间key策略(volatile-lru)**

  和`allkeys-lru`策略类似,但是只针对设置了过期时间的key进行淘汰.

* **随机删除策略(allkeys-random)**

  在达到内存最大限制时,随机删除一部分key.

* **随机删除带过期时间key策略(allkeys-random)**

  在达到内存最大限制时,随机删除一部分设置了过期时间key.

* **过期时间顺序淘汰策略(volatile-ttl)**

  根据过期时间顺序,优先删除剩余过期时间短的key.

  

一般来说:

* 如果redis被用于缓存,那么推荐volatile-lru策略.通过使用频率决定淘汰的顺序
* 如果redis被用于持久化存储业务数据的中间件(如 分布式锁 队列 基数统计 等功能).建议使用默认策略.由程序员来决定要淘汰哪些key
* 如果既有缓存,又需要持久化,那么建议使用两个不同的redis集群.

#### 设置redis内存最大限制

目的是把redis使用的内存最大值限制在一个范围内.通过`redis.conf` 进行配置

```bash
# 编辑redis配置文件，加入最大内存使用限制，我根据服务器的情况设置为3G 
maxmemory 3221225472
```

#### redis中的近似LRU算法

 `LRU` 是 `Least Recently Used` 的缩写，即最近最少使用，是一种常用的淘汰算法，选择最近最久未使用的元素予以淘汰.

LRU的实现方式很简单.把数据存储在链表里,每当访问一个元素时将它移动到链表的顶端.当元素数量超出链表的容量限制时,则将链表尾部的数据丢弃掉.`Java`里的`LinkedList`就是这么干的.



但是,对于可能存储大量元素的redis来说,使用链表的代价实在是太大了,需要巨量的存储空间.

目前redis使用的是近似LRU算法.毕竟,对于redis来说,并不需要一个完全准确的LRU算法,就算移除了一个最近访问过的Key,影响也不大.



###### redis中的近似LRU的实现方式 

当内存满了需要淘汰数据时，会调用 `dictGetSomeKeys` 选取指定的数目的key，然后更新到`eviction pool`里面.

`eviction pool`是一个数组，保存了之前随机选取的key及它们的idle时间，数组里面的key按idle时间升序排序.

如果选取的key的idle时间比`eviction pool`里面idle时间最小的key还要小，那么就不会把它插入到`eviction pool`里面. 然后淘汰掉`eviction pool`里idle时间最大的key.

通过采用`eviction pool`，把一个全局排序问题 转化成为了局部的比较问题.这个算法还是值得借鉴的.


